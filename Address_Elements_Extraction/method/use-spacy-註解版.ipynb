{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 安裝套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandarallel\n",
    "!pip install pyahocorasick\n",
    "!pip install --upgrade pip\n",
    "!pip install spacy==3.0.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()\n",
    "\n",
    "import spacy\n",
    "# spacy.require_gpu()\n",
    "from spacy.training import Example\n",
    "from spacy.util import minibatch\n",
    "import random\n",
    "\n",
    "import ahocorasick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 讀取資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../input/scl-2021-ds/train.csv\")\n",
    "df.set_index(\"id\", inplace=True)\n",
    "df['POI'] = np.nan\n",
    "df['street'] = np.nan\n",
    "\n",
    "def extract_entities(row):\n",
    "    extracted = row['POI/street'].split(\"/\")\n",
    "    \n",
    "    if len(extracted) == 2:\n",
    "        poi, street = extracted\n",
    "        if poi.strip() != '':\n",
    "            row['POI'] = poi\n",
    "        \n",
    "        if street.strip() != '':\n",
    "            row['street'] = street\n",
    "        \n",
    "    return row\n",
    "\n",
    "df = df.parallel_apply(extract_entities, axis=1)\n",
    "nlp = spacy.blank('id')  # create blank Language class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 處理資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def _build_aho(words):\n",
    "    aho = ahocorasick.Automaton()\n",
    "    for idx, key in enumerate(words):\n",
    "        aho.add_word(key, (idx, key))\n",
    "\n",
    "    return aho\n",
    "\n",
    "## 格式化data\n",
    "def format_data(text, poi, street):\n",
    "    entities = []\n",
    "    _text = deepcopy(text)\n",
    "    \n",
    "    ## 處理poi\n",
    "    if isinstance(poi, str):\n",
    "        aho = _build_aho([poi])\n",
    "        aho.make_automaton()\n",
    "        latest_char_idx = 0\n",
    "        \n",
    "        for end, (_, word) in aho.iter(_text):\n",
    "            start = end - len(word) + 1\n",
    "            if start < latest_char_idx:\n",
    "                continue\n",
    "\n",
    "            entities.append((start, end + 1, 'POI'))\n",
    "            _text = _text.replace(word, \" \" * len(word))\n",
    "            latest_char_idx = end + 1\n",
    "    \n",
    "    ## 處理street\n",
    "    if isinstance(street, str):\n",
    "        aho = _build_aho([street])\n",
    "        aho.make_automaton()\n",
    "        latest_char_idx = 0\n",
    "\n",
    "        for end, (_, word) in aho.iter(_text):\n",
    "            start = end - len(word) + 1\n",
    "            if start < latest_char_idx:\n",
    "                continue\n",
    "\n",
    "            entities.append((start, end + 1, 'STREET'))\n",
    "            latest_char_idx = end + 1\n",
    "    \n",
    "    return Example.from_dict(nlp.make_doc(text), {\"entities\": entities})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preparing Spacy examples...\")\n",
    "\n",
    "examples = []\n",
    "for idx in df.index:\n",
    "    try:\n",
    "        row = df.loc[idx]\n",
    "        example = format_data(row['raw_address'], row['POI'], row['street'])\n",
    "        examples.append(example)\n",
    "    except Exception as e:\n",
    "        print(idx)\n",
    "        print(\"-\" * 50)\n",
    "        print(e)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_spacy(nlp, examples, iterations):\n",
    "    \n",
    "    TRAIN_DATA = examples\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.add_pipe('ner', last=True)\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(iterations):\n",
    "            print(\"Starting iteration \" + str(itn))\n",
    "            random.shuffle(examples)\n",
    "            batches = minibatch(examples, 1000)\n",
    "            losses = {}\n",
    "            for batch in batches:\n",
    "                nlp.update(\n",
    "                    batch,\n",
    "                    drop=0.2,  # dropout - make it harder to memorise data\n",
    "                    sgd=optimizer,  # callable to update weights\n",
    "                    losses=losses)\n",
    "            print(losses)\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ner_nlp = train_spacy(nlp, train, 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 看預測結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df.iloc[100:110].iterrows():\n",
    "    print(f\"address: {row['raw_address']}\")\n",
    "    print(f\"expected poi: {row['POI']}\")\n",
    "    print(f\"expected street: {row['street']}\")\n",
    "    print()\n",
    "    \n",
    "    doc = ner_nlp(row['raw_address'])\n",
    "    for ent in doc.ents:\n",
    "        print(ent.text, \"-\", ent.label_)\n",
    "\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 將test資料預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"../input/scl-2021-ds/test.csv\")\n",
    "df_test.set_index(\"id\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = []\n",
    "for idx, row in df_test.iloc[:].iterrows():\n",
    "    doc = ner_nlp(row['raw_address'])\n",
    "    tmp = {'id': idx}\n",
    "    for ent in doc.ents:\n",
    "        tmp[ent.label_] = ent.text\n",
    "    submission.append(tmp)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(submission)\n",
    "submission['POI'] = submission['POI'].replace(np.nan, '')\n",
    "submission['STREET'] = submission['STREET'].replace(np.nan, '')\n",
    "submission['POI/street'] = submission['POI'] + '/' + submission['STREET'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'id':submission['id'],'POI/street':submission['POI/street']}).to_csv('submission.csv', header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
